{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17137,
     "status": "ok",
     "timestamp": 1753098001865,
     "user": {
      "displayName": "TH·ªäNH TR·∫¶N H∆ØNG",
      "userId": "12514201472503842251"
     },
     "user_tz": -420
    },
    "id": "0YGZXJEIpR6b",
    "outputId": "6ababfb5-5a31-46ea-e92a-aec734d22e9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Downloading boto3-1.39.9-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting botocore<1.40.0,>=1.39.9 (from boto3)\n",
      "  Downloading botocore-1.39.9-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3)\n",
      "  Downloading s3transfer-0.13.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore<1.40.0,>=1.39.9->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.40.0,>=1.39.9->boto3) (2.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.40.0,>=1.39.9->boto3) (1.17.0)\n",
      "Downloading boto3-1.39.9-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.39.9-py3-none-any.whl (13.9 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading s3transfer-0.13.1-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: jmespath, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.39.9 botocore-1.39.9 jmespath-1.0.1 s3transfer-0.13.1\n"
     ]
    }
   ],
   "source": [
    "pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "filename = \"cardio_clean.csv\"\n",
    "df_predict.to_csv(filename, index=False)\n",
    "\n",
    "aws_access_key = 'xxx'\n",
    "aws_secret_key = 'xxx'\n",
    "region = 'ap-southeast-1'\n",
    "bucket_name = 'public-bucket-nhuttan-01'\n",
    "\n",
    "\n",
    "s3 = boto3.client('s3',\n",
    "                  aws_access_key_id=aws_access_key,\n",
    "                  aws_secret_access_key=aws_secret_key,\n",
    "                  region_name=region)\n",
    "\n",
    "s3.upload_file(Filename=filename,\n",
    "               Bucket=bucket_name,\n",
    "               Key=f\"data/processed/{filename}\")\n",
    "\n",
    "print(f\" ƒê√£ upload {filename} l√™n bucket `{bucket_name}`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oN0qr4ccBrXu"
   },
   "source": [
    "#Kh·ªüi t·∫°o SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9n6h4qjGBpHo"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Cardio Prediction\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCZlyXA6BzOG"
   },
   "source": [
    "# ƒê·ªçc d·ªØ li·ªáu t·ª´ file CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12717,
     "status": "ok",
     "timestamp": 1753098033550,
     "user": {
      "displayName": "TH·ªäNH TR·∫¶N H∆ØNG",
      "userId": "12514201472503842251"
     },
     "user_tz": -420
    },
    "id": "cX0gbPplBx3z",
    "outputId": "8e902d4a-8420-4223-9080-76ea302fc6f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age_years: integer (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- ap_hi: double (nullable = true)\n",
      " |-- ap_lo: integer (nullable = true)\n",
      " |-- cholesterol: integer (nullable = true)\n",
      " |-- cardio: integer (nullable = true)\n",
      " |-- bmi: double (nullable = true)\n",
      "\n",
      "+---------+------+------+-----+-----+-----------+------+------------------+\n",
      "|age_years|height|weight|ap_hi|ap_lo|cholesterol|cardio|               bmi|\n",
      "+---------+------+------+-----+-----+-----------+------+------------------+\n",
      "|       50|   168|  62.0|110.0|   80|          1|     0|  21.9671201814059|\n",
      "|       55|   156|  85.0|140.0|   90|          3|     1|34.927679158448385|\n",
      "|       51|   165|  64.0|130.0|   70|          3|     1|23.507805325987146|\n",
      "|       48|   169|  82.0|150.0|  100|          1|     1| 28.71047932495361|\n",
      "|       47|   156|  56.0|100.0|   60|          1|     0|23.011176857330703|\n",
      "+---------+------+------+-----+-----+-----------+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark = spark.read.csv(\"cardio_clean.csv\", header=True, inferSchema=True)\n",
    "df_spark.printSchema()\n",
    "df_spark.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0seXBUedCDgB"
   },
   "source": [
    "##T·∫°o danh s√°ch c√°c c·ªôt feature ƒë·∫ßu v√†o\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0c0DZzQB7L6"
   },
   "outputs": [],
   "source": [
    "feature_cols = ['age_years', 'ap_hi', 'ap_lo', 'cholesterol', 'bmi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqBzC5SECO1O"
   },
   "source": [
    "#VectorAssembler: g·ªôp c√°c c·ªôt feature l·∫°i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l334aX4UCMO3"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"assembled_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LiYINKNwCddB"
   },
   "source": [
    "#StandardScaler: chu·∫©n h√≥a d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3qezQroCZke"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol=\"assembled_features\", outputCol=\"features\", withMean=True, withStd=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Krotm-waCiXu"
   },
   "source": [
    "#T√°ch train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6HRJhDmZChw1"
   },
   "outputs": [],
   "source": [
    "train_data, test_data = df_spark.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVzIFyO7j-tW"
   },
   "source": [
    "# H√†m create_pipeline(model, use_scaler=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p9O-97ERp9MO"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "def create_pipeline(model, use_scaler=True):\n",
    "    stages = [assembler]\n",
    "    if use_scaler:\n",
    "        stages.append(scaler)\n",
    "        model.setFeaturesCol(\"features\")\n",
    "    else:\n",
    "        model.setFeaturesCol(\"assembled_features\")\n",
    "\n",
    "    model.setLabelCol(\"cardio\")\n",
    "    stages.append(model)\n",
    "\n",
    "    return Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_yxo3PTFYbKN"
   },
   "source": [
    "#H√†m ƒë√°nh gi√° m·ªôt m√¥ h√¨nh ƒë√£ predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q2nVBateqGv_"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "def evaluate_model(predictions, model_name=\"M√¥ h√¨nh\"):\n",
    "    evaluator_acc = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"cardio\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    "    )\n",
    "    evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"cardio\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    "    )\n",
    "\n",
    "    acc = evaluator_acc.evaluate(predictions)\n",
    "    f1 = evaluator_f1.evaluate(predictions)\n",
    "\n",
    "    print(f\" {model_name}\")\n",
    "    print(f\" Accuracy: {acc:.2%}\")\n",
    "    print(f\" F1-score: {f1:.2%}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTqIM_17YdS0"
   },
   "source": [
    "#LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19517,
     "status": "ok",
     "timestamp": 1753098053978,
     "user": {
      "displayName": "TH·ªäNH TR·∫¶N H∆ØNG",
      "userId": "12514201472503842251"
     },
     "user_tz": -420
    },
    "id": "9T6HjbEjqLG6",
    "outputId": "1869d5aa-bf2a-419b-ebd3-bcc508d5e321"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " üîç Logistic Regression\n",
      " Accuracy: 73.05%\n",
      " F1-score: 72.94%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"assembled_features\")\n",
    "scaler = StandardScaler(inputCol=\"assembled_features\", outputCol=\"features\", withMean=True, withStd=True)\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"cardio\", maxIter=100)\n",
    "\n",
    "pipeline_lr = Pipeline(stages=[assembler, scaler, lr])\n",
    "\n",
    "lr_model = pipeline_lr.fit(train_data)\n",
    "\n",
    "lr_predictions = lr_model.transform(test_data)\n",
    "evaluate_model(lr_predictions, \"üîç Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYB0Kzh-PGsU"
   },
   "source": [
    "#Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8231,
     "status": "ok",
     "timestamp": 1753098062201,
     "user": {
      "displayName": "TH·ªäNH TR·∫¶N H∆ØNG",
      "userId": "12514201472503842251"
     },
     "user_tz": -420
    },
    "id": "1EXwn89lqb-P",
    "outputId": "79acdea0-c662-4b57-a4b9-de43a8425e3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Decision Tree\n",
      " Accuracy: 73.38%\n",
      " F1-score: 73.30%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "pipeline_dt = create_pipeline(dt, use_scaler=False)\n",
    "dt_model = pipeline_dt.fit(train_data)\n",
    "dt_predictions = dt_model.transform(test_data)\n",
    "evaluate_model(dt_predictions, \"Decision Tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vj8UQt5TYiLD"
   },
   "source": [
    "#Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54842,
     "status": "ok",
     "timestamp": 1753098117044,
     "user": {
      "displayName": "TH·ªäNH TR·∫¶N H∆ØNG",
      "userId": "12514201472503842251"
     },
     "user_tz": -420
    },
    "id": "I3eVZ4fHqlM3",
    "outputId": "6c029c64-befd-498f-e8cc-87517992367e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Gradient Boosted Trees\n",
      " Accuracy: 73.63%\n",
      " F1-score: 73.57%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(maxIter=50)\n",
    "pipeline_gbt = create_pipeline(gbt, use_scaler=False)\n",
    "gbt_model = pipeline_gbt.fit(train_data)\n",
    "gbt_predictions = gbt_model.transform(test_data)\n",
    "evaluate_model(gbt_predictions, \"Gradient Boosted Trees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4o2JD7uaPkSl"
   },
   "source": [
    "#Linear SVM (Support Vector Machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12008,
     "status": "ok",
     "timestamp": 1753098129042,
     "user": {
      "displayName": "TH·ªäNH TR·∫¶N H∆ØNG",
      "userId": "12514201472503842251"
     },
     "user_tz": -420
    },
    "id": "NHb69wXiq4bK",
    "outputId": "d78bb8bf-64e1-4279-ea31-98579ce5fe2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Linear SVM\n",
      " Accuracy: 72.94%\n",
      " F1-score: 72.70%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "svm = LinearSVC(maxIter=100)\n",
    "pipeline_svm = create_pipeline(svm, use_scaler=True)\n",
    "svm_model = pipeline_svm.fit(train_data)\n",
    "svm_predictions = svm_model.transform(test_data)\n",
    "evaluate_model(svm_predictions, \"Linear SVM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xF0B1UuNPi0K"
   },
   "source": [
    "#Multilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17096,
     "status": "ok",
     "timestamp": 1753098146148,
     "user": {
      "displayName": "TH·ªäNH TR·∫¶N H∆ØNG",
      "userId": "12514201472503842251"
     },
     "user_tz": -420
    },
    "id": "M80FmSp6rIRR",
    "outputId": "931a9565-3433-4f46-9fae-83ccd150b56e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Multilayer Perceptron (MLP)\n",
      " Accuracy: 73.68%\n",
      " F1-score: 73.63%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "\n",
    "layers = [5, 5, 2]\n",
    "\n",
    "mlp = MultilayerPerceptronClassifier(maxIter=100, layers=layers, seed=42, featuresCol=\"features\", labelCol=\"cardio\")\n",
    "\n",
    "pipeline_mlp = create_pipeline(mlp, use_scaler=True)\n",
    "mlp_model = pipeline_mlp.fit(train_data)\n",
    "mlp_predictions = mlp_model.transform(test_data)\n",
    "\n",
    "evaluate_model(mlp_predictions, \"Multilayer Perceptron (MLP)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrRioRFePXxy"
   },
   "source": [
    "#M√¥ h√¨nh One-vs-Rest v·ªõi Linear SVM (OvR-SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25037,
     "status": "ok",
     "timestamp": 1753098171181,
     "user": {
      "displayName": "TH·ªäNH TR·∫¶N H∆ØNG",
      "userId": "12514201472503842251"
     },
     "user_tz": -420
    },
    "id": "D8slOTp0rYc-",
    "outputId": "e9bb0f46-d4e3-4efa-8604-e1fe007b97f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One-vs-Rest (Linear SVM)\n",
      " Accuracy: 72.94%\n",
      " F1-score: 72.70%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import OneVsRest\n",
    "\n",
    "# S·ª≠ d·ª•ng LinearSVC l√†m base classifier\n",
    "ovr = OneVsRest(classifier=LinearSVC(maxIter=100))\n",
    "pipeline_ovr = create_pipeline(ovr, use_scaler=True)\n",
    "\n",
    "ovr_model = pipeline_ovr.fit(train_data)\n",
    "ovr_predictions = ovr_model.transform(test_data)\n",
    "\n",
    "evaluate_model(ovr_predictions, \"One-vs-Rest (Linear SVM)\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
